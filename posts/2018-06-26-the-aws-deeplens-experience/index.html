<!doctype html><html lang=en><head><title>The AWS DeepLens Experience :: Impulsive Robotics</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="I had a great time at Intel AI DevCon 2018. They gave out a few goodies which included an AWS DeepLens preorder code. To be honest, I didn’t really understand the point of it when I preordered it, but I did it anyway since it was free. I didn’t look at the specs till it arrived. I was expecting a device locked into using AWS apps. But boy was I wrong."><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://www.impulsiverobotics.com/posts/2018-06-26-the-aws-deeplens-experience/><link rel=stylesheet href=https://www.impulsiverobotics.com/assets/style.css><link rel=stylesheet href=assets/%25!s%28%3cnil%3e%29.css><link rel=apple-touch-icon href=https://www.impulsiverobotics.com/img/apple-touch-icon-192x192.png><link rel="shortcut icon" href=https://www.impulsiverobotics.com/img/favicon/orange.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="The AWS DeepLens Experience"><meta property="og:description" content="I had a great time at Intel AI DevCon 2018. They gave out a few goodies which included an AWS DeepLens preorder code. To be honest, I didn’t really understand the point of it when I preordered it, but I did it anyway since it was free. I didn’t look at the specs till it arrived. I was expecting a device locked into using AWS apps. But boy was I wrong."><meta property="og:url" content="https://www.impulsiverobotics.com/posts/2018-06-26-the-aws-deeplens-experience/"><meta property="og:site_name" content="Impulsive Robotics"><meta property="og:image" content="img/favicon/%!s().png"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="1024"><meta property="article:section" content="Posts"><meta property="article:published_time" content="2018-06-26 06:33:27 +0000 UTC"></head><body><div class="container headings--one-size"><header class=header><div class=header__inner><div class=header__logo><a href=https://www.impulsiverobotics.com><div class=logo>Terminal</div></a></div><div class=menu-trigger>menu</div></div><nav class=menu><ul class="menu__inner menu__inner--desktop"><li><a href=/about/>about</a></li></ul><ul class="menu__inner menu__inner--mobile"><li><a href=/about/>about</a></li></ul></nav></header><div class=content><div class=post><h1 class=post-title><a href=https://www.impulsiverobotics.com/posts/2018-06-26-the-aws-deeplens-experience/>The AWS DeepLens Experience</a></h1><div class=post-meta><span class=post-date>2018-06-26</span>
<span class=post-author>:: Nagarjun Redla</span></div><div class=post-content><div><p>I had a great time at Intel AI DevCon 2018. They gave out a few goodies which included an AWS DeepLens preorder code. To be honest, I didn’t really understand the point of it when I preordered it, but I did it anyway since it was free. I didn’t look at the specs till it arrived. I was expecting a device locked into using AWS apps. But boy was I wrong.</p><h2 id=first-impressions>First Impressions<a href=#first-impressions class=hanchor arialabel=Anchor>&#8983;</a></h2><p>When the DeepLens first arrived, I was surprised to see how big it was. I’d expected it to be smaller, I guess? The Intel Inside badge is what separates this thing from other common developer devices. The device is pretty light and uses a 5V 4A power supply.</p><p><img src="https://i0.wp.com/impulsiverobotics.com/wp-content/uploads/2018/06/IMG_20180626_170815.jpg?resize=1024%2C768&ssl=1" alt="Deeplens and some amazon boxes"></p><h3 id=tech-specs>Tech Specs<a href=#tech-specs class=hanchor arialabel=Anchor>&#8983;</a></h3><p>This thing has a Dual Core Intel Atom E3930, 8GB RAM and 16GB in-built memory. Compared to the UP Core, it’s two cores lesser but 3 Quarters newer, so it had a bunch of standardized security stuff. A pretty powerful OOTB dev tool if you ask me.</p><iframe allowfullscreen frameborder=0 height=281.25 scrolling=no src=https://gfycat.com/ifr/goodnatureddefenselessbongo width=500></iframe><h3 id=io>I/O<a href=#io class=hanchor arialabel=Anchor>&#8983;</a></h3><p><img src="https://i0.wp.com/impulsiverobotics.com/wp-content/uploads/2018/06/IMG_20180626_170959.jpg?resize=320%2C427&ssl=1" alt="Deeplens IO"></p><p>The DeepLens has a ton of I/O that you can make use of. It has a micro SD card slot, a micro HDMI, an audio line out and two very misleading USB2.0 ports that are blue like USB3.0. A quick <code>lsusb</code> shows that the device is capable of USB3.0, but the camera and the USB ports on the back are on the USB2.0 bus. The AWS component of this device makes it easy to use without a monitor, but I’d recommend a micro HDMI adapter for this.</p><h3 id=software>Software<a href=#software class=hanchor arialabel=Anchor>&#8983;</a></h3><p>Onto the bigger questions. How locked is it? When setting it up, I thought I followed the instructions wrong (or forgot the password I assigned to it) and lost access to the device. I quickly looked up recovery methods and found <a href=https://s3.amazonaws.com/deeplens-public/factory-restore/DeepLens_System_Restore_Instruction.pdf>this</a>, which asks the user to make a bootable Ubuntu 16.04 flash drive along with an SD card with AWS stuff. Looking at that made me realize this isn’t really locked much. This is a full x86 computer with a good camera. You could use the device without the AWS component but I think you’d lose access to the camera.</p><p>The DeepLens comes with common frameworks and libraries installed, like OpenCV. The main theme of Intel AI DevCon 2018 was Intel showing off how they’ve optimized the Machine Learning Inference process to run faster on Intel chipsets. They kept pushing the Model Optimizer and Inference Engine included in the OpenVINO toolkit (formerly known as deep learning deployment toolkit) and how you can leverage AI on the edge. The OpenVINO toolkit and Inference Engine documentation talk about Greengrass and lambdas, so I’m assuming this is included.</p><p>Looking at the <code>/opt/</code> folder, I found <code>/opt/intel/</code> and <code>/opt/aws_cam/</code> . The Intel folder contained the Deep Learning Deployment Toolkit and the aws_cam folder included the webserver that streams camera data. Upon close inspection we see that the camera live feed can be found at <code>/opt/aws_cam/out/ch1_out.h264</code> and <code>/opt/aws_cam/out/ch2_out.mjpeg</code>. This should be enough to run a simple demo.</p><h2 id=demo>Demo<a href=#demo class=hanchor arialabel=Anchor>&#8983;</a></h2><p>The workflow is simple. Train a model in TensorFlow, Caffe or MXNet (at the moment), then for inference, we need to convert the saved model to what Intel calls Intermediate Representation files (.xml and .bin) using the Model Optimizer on the Development Platform. These files are then fed to the Inference Engine on your Target Platform (device). The <a href=https://software.intel.com/en-us/openvino-toolkit>OpenVINO toolkit</a> doesn’t include Intel Atom under Development Platform, but I tried and it worked.</p><h3 id=model-optimizer>Model Optimizer<a href=#model-optimizer class=hanchor arialabel=Anchor>&#8983;</a></h3><p>The Deep Learning Deployment Toolkit only includes the Model Optimizer for MXNet models, so that’s no good. I went ahead and installed OpenVINO directly. This includes Model Optimizer for Caffe and TensorFlow models.</p><p>Download the <a href=https://software.intel.com/file/609199/download>Caffe GoogLeNet SSD model</a> provided by Intel and unzip it. Then run the model optimizer using</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>python3 /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py --input_model ~/Downloads/SSD_GoogleNetV2.caffemodel --input_proto ~/Downloads/SSD_GoogleNetV2_Deploy.prototxt --output_dir ~/Downloads/
</span></span></code></pre></div><p>This generates two IR (Intermediate Representation) files <code>SSD_GoogleNetV2.xml</code> and <code>SSD_GoogleNetV2.bin</code> in the Downloads folder.</p><h3 id=inference-engine>Inference Engine<a href=#inference-engine class=hanchor arialabel=Anchor>&#8983;</a></h3><p>There are many samples included in the inference engine directory. I’ll try the object detection SSD sample. First, build the examples.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>cd /opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples
</span></span><span style=display:flex><span>cmake .
</span></span><span style=display:flex><span>sudo make install
</span></span><span style=display:flex><span>cd intel64/Release
</span></span></code></pre></div><p>Now run the Object Detection Demo SSD Async using</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./object_detection_demo_ssd_async -i /opt/awscam/out/ch2_out.mjpeg -m ~/Downloads/SSD_GoogleNetV2.xml -d CPU
</span></span></code></pre></div><p>You should see a stream with predictions. The frame rate averages between 1 and 2 frames per second.</p><p><figure><img src="https://i0.wp.com/impulsiverobotics.com/wp-content/uploads/2018/06/screenshot.png?resize=1024%2C576&ssl=1" alt="SSD Inference example"><figcaption>Inference Engine SSD Sample</figcaption></figure></p><h2 id=conclusion>Conclusion<a href=#conclusion class=hanchor arialabel=Anchor>&#8983;</a></h2><p>This is a very cool tool to have. I had trouble getting a screenshot of better fps performance, since performing any other task, like taking a screenshot, hangs the application for a few seconds. It is critically designed to handle the Inference Engine and AWS lambda and greengrass. I wouldn’t use it for timing critical applications, in such cases it’s better to offload inference to the Movidius NCS anyway. But to try out vision based deep learning at home, this is awesome. The toolkit currently under active development. The Python SDK was in preview when I got the DeepLens but it is now available in a newer release. Unlike my usual problem with running out of ports on SBCs, the DeepLens has a few free ports. Let’s see how I can integrate this into a project.</p></div></div></div></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2022 Powered by <a href=http://gohugo.io>Hugo</a></span>
<span>:: Theme made by <a href=https://twitter.com/panr>panr</a></span></div></div></footer><script src=https://www.impulsiverobotics.com/assets/main.js></script>
<script src=https://www.impulsiverobotics.com/assets/prism.js></script></div></body></html>