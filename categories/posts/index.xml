<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Impulsive Robotics</title><link>https://nagarjunredla.github.io/impulsiverobotics/categories/posts/</link><description>Recent content in Posts on Impulsive Robotics</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 02 Sep 2018 02:01:52 +0000</lastBuildDate><atom:link href="https://nagarjunredla.github.io/impulsiverobotics/categories/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>My Donkeycar Build experience</title><link>https://nagarjunredla.github.io/impulsiverobotics/posts/2018-09-02-my-donkeycar-build-experience/</link><pubDate>Sun, 02 Sep 2018 02:01:52 +0000</pubDate><guid>https://nagarjunredla.github.io/impulsiverobotics/posts/2018-09-02-my-donkeycar-build-experience/</guid><description>I’ve been interested in building a self-navigating robot for a while from hobby electronics. The problem was that I tried adding to the complexity by taking too many variables into account. I tried multiple differential drive kits only to realize some tiny part about it wouldn’t make it feasible to use. I will revisit these issues in a build I’ve been working on for a while, but wanted to mention this to explain why this isn’t under the “builds” category.</description><content>&lt;p>I’ve been interested in building a self-navigating robot for a while from hobby electronics. The problem was that I tried adding to the complexity by taking too many variables into account. I tried multiple differential drive kits only to realize some tiny part about it wouldn’t make it feasible to use. I will revisit these issues in a build I’ve been working on for a while, but wanted to mention this to explain why this isn’t under the “builds” category. The Redbot Extreme was almost okay but it wasn’t Akerman.&lt;/p>
&lt;p>Learning from earlier mistakes, I wasn’t going to make another one by trying to build a RC car with Ackerman steering from the ground up. I remember watching &lt;a href="https://www.youtube.com/watch?v=tOj53RRgtmg">a video by Hackaday&lt;/a> where they mentioned the Donkeycar, so I thought I’d give it a try.&lt;/p>
&lt;p>The build is very straightforward, but sourcing the base is not. This build was going to be as hassle free as possible to I sourced whatever &lt;a href="https://squareup.com/store/donkeycar">donkeycar sold&lt;/a> on their site but couldn’t find the recommended RC car in stock. Newegg let me order one but the supplier immediately cancelled the order. So I went for the Exceed Desert Monster build, which offers its own set of advantages.&lt;/p>
&lt;!-- raw HTML omitted --></content></item><item><title>The AWS DeepLens Experience</title><link>https://nagarjunredla.github.io/impulsiverobotics/posts/2018-06-26-the-aws-deeplens-experience/</link><pubDate>Tue, 26 Jun 2018 06:33:27 +0000</pubDate><guid>https://nagarjunredla.github.io/impulsiverobotics/posts/2018-06-26-the-aws-deeplens-experience/</guid><description>I had a great time at Intel AI DevCon 2018. They gave out a few goodies which included an AWS DeepLens preorder code. To be honest, I didn’t really understand the point of it when I preordered it, but I did it anyway since it was free. I didn’t look at the specs till it arrived. I was expecting a device locked into using AWS apps. But boy was I wrong.</description><content>&lt;p>I had a great time at Intel AI DevCon 2018. They gave out a few goodies which included an AWS DeepLens preorder code. To be honest, I didn’t really understand the point of it when I preordered it, but I did it anyway since it was free. I didn’t look at the specs till it arrived. I was expecting a device locked into using AWS apps. But boy was I wrong.&lt;/p>
&lt;h2 id="first-impressions">First Impressions&lt;/h2>
&lt;p>When the DeepLens first arrived, I was surprised to see how big it was. I’d expected it to be smaller, I guess? The Intel Inside badge is what separates this thing from other common developer devices. The device is pretty light and uses a 5V 4A power supply.&lt;/p>
&lt;p>&lt;img src="https://i0.wp.com/impulsiverobotics.com/wp-content/uploads/2018/06/IMG_20180626_170815.jpg?resize=1024%2C768&amp;amp;ssl=1" alt="">&lt;/p>
&lt;h3 id="tech-specs">Tech Specs&lt;/h3>
&lt;p>This thing has a Dual Core Intel Atom E3930, 8GB RAM and 16GB in-built memory. Compared to the UP Core, it’s two cores lesser but 3 Quarters newer, so it had a bunch of standardized security stuff. A pretty powerful OOTB dev tool if you ask me.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h3 id="io">I/O&lt;/h3>
&lt;p>&lt;img src="https://i0.wp.com/impulsiverobotics.com/wp-content/uploads/2018/06/IMG_20180626_170959.jpg?resize=320%2C427&amp;amp;ssl=1" alt="">&lt;/p>
&lt;p>The DeepLens has a ton of I/O that you can make use of. It has a micro SD card slot, a micro HDMI, an audio line out and two very misleading USB2.0 ports that are blue like USB3.0. A quick &lt;code>lsusb&lt;/code> shows that the device is capable of USB3.0, but the camera and the USB ports on the back are on the USB2.0 bus. The AWS component of this device makes it easy to use without a monitor, but I’d recommend a micro HDMI adapter for this.&lt;/p>
&lt;h3 id="software">Software&lt;/h3>
&lt;p>Onto the bigger questions. How locked is it? When setting it up, I thought I followed the instructions wrong (or forgot the password I assigned to it) and lost access to the device. I quickly looked up recovery methods and found &lt;a href="https://s3.amazonaws.com/deeplens-public/factory-restore/DeepLens_System_Restore_Instruction.pdf">this&lt;/a>, which asks the user to make a bootable Ubuntu 16.04 flash drive along with an SD card with AWS stuff. Looking at that made me realize this isn’t really locked much. This is a full x86 computer with a good camera. You could use the device without the AWS component but I think you’d lose access to the camera.&lt;/p>
&lt;p>The DeepLens comes with common frameworks and libraries installed, like OpenCV. The main theme of Intel AI DevCon 2018 was Intel showing off how they’ve optimized the Machine Learning Inference process to run faster on Intel chipsets. They kept pushing the Model Optimizer and Inference Engine included in the OpenVINO toolkit (formerly known as deep learning deployment toolkit) and how you can leverage AI on the edge. The OpenVINO toolkit and Inference Engine documentation talk about Greengrass and lambdas, so I’m assuming this is included.&lt;/p>
&lt;p>Looking at the &lt;code>/opt/&lt;/code> folder, I found &lt;code>/opt/intel/&lt;/code> and &lt;code>/opt/aws_cam/&lt;/code> . The Intel folder contained the Deep Learning Deployment Toolkit and the aws_cam folder included the webserver that streams camera data. Upon close inspection we see that the camera live feed can be found at &lt;code>/opt/aws_cam/out/ch1_out.h264&lt;/code> and &lt;code>/opt/aws_cam/out/ch2_out.mjpeg&lt;/code>. This should be enough to run a simple demo.&lt;/p>
&lt;h2 id="demo">Demo&lt;/h2>
&lt;p>The workflow is simple. Train a model in TensorFlow, Caffe or MXNet (at the moment), then for inference, we need to convert the saved model to what Intel calls Intermediate Representation files (.xml and .bin) using the Model Optimizer on the Development Platform. These files are then fed to the Inference Engine on your Target Platform (device). The &lt;a href="https://software.intel.com/en-us/openvino-toolkit">OpenVINO toolkit&lt;/a> doesn’t include Intel Atom under Development Platform, but I tried and it worked.&lt;/p>
&lt;h3 id="model-optimizer">Model Optimizer&lt;/h3>
&lt;p>The Deep Learning Deployment Toolkit only includes the Model Optimizer for MXNet models, so that’s no good. I went ahead and installed OpenVINO directly. This includes Model Optimizer for Caffe and TensorFlow models.&lt;/p>
&lt;p>Download the &lt;a href="https://software.intel.com/file/609199/download">Caffe GoogLeNet SSD model&lt;/a> provided by Intel and unzip it. Then run the model optimizer using&lt;/p>
&lt;pre tabindex="0">&lt;code>python3 /opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py --input_model ~/Downloads/SSD_GoogleNetV2.caffemodel --input_proto ~/Downloads/SSD_GoogleNetV2_Deploy.prototxt --output_dir ~/Downloads/
&lt;/code>&lt;/pre>&lt;p>This generates two IR (Intermediate Representation) files &lt;code>SSD_GoogleNetV2.xml&lt;/code> and &lt;code>SSD_GoogleNetV2.bin&lt;/code> in the Downloads folder.&lt;/p>
&lt;h3 id="inference-engine">Inference Engine&lt;/h3>
&lt;p>There are many samples included in the inference engine directory. I’ll try the object detection SSD sample. First, build the examples.&lt;/p>
&lt;pre tabindex="0">&lt;code>cd /opt/intel/computer_vision_sdk/deployment_tools/inference_engine/samples
cmake .
sudo make install
cd intel64/Release
&lt;/code>&lt;/pre>&lt;p>Now run the Object Detection Demo SSD Async using&lt;/p>
&lt;pre tabindex="0">&lt;code>./object_detection_demo_ssd_async -i /opt/awscam/out/ch2_out.mjpeg -m ~/Downloads/SSD_GoogleNetV2.xml -d CPU
&lt;/code>&lt;/pre>&lt;p>You should see a stream with predictions. The frame rate averages between 1 and 2 frames per second.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>This is a very cool tool to have. I had trouble getting a screenshot of better fps performance, since performing any other task, like taking a screenshot, hangs the application for a few seconds. It is critically designed to handle the Inference Engine and AWS lambda and greengrass. I wouldn’t use it for timing critical applications, in such cases it’s better to offload inference to the Movidius NCS anyway. But to try out vision based deep learning at home, this is awesome. The toolkit currently under active development. The Python SDK was in preview when I got the DeepLens but it is now available in a newer release. Unlike my usual problem with running out of ports on SBCs, the DeepLens has a few free ports. Let’s see how I can integrate this into a project.&lt;/p></content></item></channel></rss>